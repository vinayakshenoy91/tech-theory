@SpringBootTest
@Testcontainers
class SampleTests {
  @Container
  private static MySQLContainer database = 
    new MySQLContainer("mysql:5.7.32");


Single Container Pattern:

To avoid this extra latency, we can use the Single Container Pattern (see https://www.testcontainers.org/test_framework_integration/manual_lifecycle_control/#singleton-containers). Following this pattern, a base class is used to launch a single Docker container for MySQL. The base class, MySqlTestBase, used in the Review microservice looks like this:

public abstract class MySqlTestBase {

  private static MySQLContainer database =
    new MySQLContainer("mysql:5.7.32");
  
  static {
    database.start();
  }

  @DynamicPropertySource
  static void databaseProperties(DynamicPropertyRegistry registry) {
    registry.add("spring.datasource.url", database::getJdbcUrl);
    registry.add("spring.datasource.username", database::getUsername);
    registry.add("spring.datasource.password", database::getPassword);
  }
}

@DataJpaTest
@AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.NONE)
class PersistenceTests extends MySqlTestBase {
class ReviewServiceApplicationTests extends MySqlTestBase { 

@DataMongoTest(
  excludeAutoConfiguration = EmbeddedMongoAutoConfiguration.class)
class PersistenceTests extends MongoDbTestBase {

----

//Writing tests
@DataMongoTest
class PersistenceTests {

    @Autowired
    private ProductRepository repository;
    private ProductEntity savedEntity;

    @BeforeEach
    void setupDb() {
        repository.deleteAll();
        ProductEntity entity = new ProductEntity(1, "n", 1);
        savedEntity = repository.save(entity);
        assertEqualsProduct(entity, savedEntity);
    }


@Test
void create() {
    ProductEntity newEntity = new ProductEntity(2, "n", 2);
    repository.save(newEntity);

    ProductEntity foundEntity = 
    repository.findById(newEntity.getId()).get();
    assertEqualsProduct(newEntity, foundEntity);

    assertEquals(2, repository.count());
}


@Test
void delete() {
    repository.delete(savedEntity);
    assertFalse(repository.existsById(savedEntity.getId()));
}

@Test
void update() {
    savedEntity.setName("n2");
    repository.save(savedEntity);

    ProductEntity foundEntity = 
    repository.findById(savedEntity.getId()).get();
    assertEquals(1, (long)foundEntity.getVersion());
    assertEquals("n2", foundEntity.getName());
}


@Test
void getByProductId() {
    Optional<ProductEntity> entity = 
    repository.findByProductId(savedEntity.getProductId());
    assertTrue(entity.isPresent());
    assertEqualsProduct(savedEntity, entity.get());
}


@Test
void duplicateError() {
  assertThrows(DuplicateKeyException.class, () -> {
    ProductEntity entity = new ProductEntity(savedEntity.getProductId(), "n", 1);
    repository.save(entity);
  });
} 


6
Adding Persistence
In this chapter, we will learn how to persist data that a microservice is using. As already mentioned in Chapter 2, Introduction to Spring Boot, we will use the Spring Data project to persist data to MongoDB and MySQL databases.

The product and recommendation microservices will use Spring Data for MongoDB and the review microservice will use Spring Data for the JPA (short for the Java Persistence API) to access a MySQL database. We will add operations to the RESTful APIs to be able to create and delete data in the databases. The existing APIs for reading data will be updated to access the databases. We will run the databases as Docker containers, managed by Docker Compose, that is, in the same way as we run our microservices.

The following topics will be covered in this chapter:

Adding a persistence layer to the core microservices
Writing automated tests that focus on persistence
Using the persistence layer in the service layer
Extending the composite service API
Adding databases to the Docker Compose landscape
Manual testing of the new APIs and the persistence layer
Updating the automated tests of the microservice landscape
Technical requirements
For instructions on how to install the tools used in this book and how to access the source code for this book, see:

Chapter 21 for macOS
Chapter 22 for Windows
To access the databases manually, we will use the CLI tools provided in the Docker images used to run the databases. We will also expose the standard ports used for each database in Docker Compose, 3306 for MySQL and 27017 for MongoDB. This will enable us to use our favorite database tools for accessing the databases in the same way as if they were running locally on our computers.

The code examples in this chapter all come from the source code in $BOOK_HOME/Chapter06.

If you want to view the changes applied to the source code in this chapter, that is, see what it took to add persistence to the microservices using Spring Data, you can compare it with the source code for Chapter 5, Adding an API Description Using OpenAPI. You can use your favorite diff tool and compare the two folders, $BOOK_HOME/Chapter05 and $BOOK_HOME/Chapter06.

Before going into details, let's see where we are heading.

Chapter objectives
By the end of this chapter, we will have layers inside our microservices that look like the following:


Figure 6.1: The microservice landscape we're aiming for

The Protocol layer handles protocol-specific logic. It is very thin, only consisting of the RestController annotations in the api project and the common GlobalControllerExceptionHandler in the util project. The main functionality of each microservice resides in the Service layers. The product-composite service contains an Integration layer used to handle the communication with the three core microservices. The core microservices will all have a Persistence layer used for communicating with their databases.

We will be able to access data stored in MongoDB with a command like the following:

docker-compose exec mongodb mongo product-db --quiet --eval "db.products.find()"
The result of the command should look like the following:


Figure 6.2: Accessing data stored in MongoDB

Regarding data stored in MySQL, we will be able to access it with a command like this:

docker-compose exec mysql mysql -uuser -p review-db -e "select * from reviews"
The result of the command should look as follows:


Figure 6.3: Accessing data stored in MySQL

The output from the mongo and mysql commands has been shortened for improved readability.

Let's see how to implement this. We will start with adding persistence functionality to our core microservices!

Adding a persistence layer to the core microservices
Let's start with adding a persistence layer to the core microservices. Besides using Spring Data, we will also use a Java bean mapping tool, MapStruct, that makes it easy to transform between Spring Data entity objects and the API model classes. For further details, see http://mapstruct.org/.

First, we need to add dependencies to MapStruct, Spring Data, and the JDBC drivers for the databases we intend to use. After that, we can define our Spring Data entity classes and repositories. The Spring Data entity classes and repositories will be placed in their own Java package, persistence. For example, for the product microservice, they will be placed in the Java package se.magnus.microservices.core.product.persistence.

Adding dependencies
We will use MapStruct v1.3.1, so we start by defining a variable holding the version information in the build file for each core microservice, build.gradle:

ext {
  mapstructVersion = "1.3.1"
}
Next, we declare a dependency on MapStruct:

implementation "org.mapstruct:mapstruct:${mapstructVersion}"
Since MapStruct generates the implementation of the bean mappings at compile time by processing MapStruct annotations, we need to add an annotationProcessor and a testAnnotationProcessor dependency:

annotationProcessor "org.mapstruct:mapstruct-processor:${mapstructVersion}"
testAnnotationProcessor "org.mapstruct:mapstruct-processor:${mapstructVersion}"
To make the compile-time generation work in popular IDEs such as IntelliJ IDEA, we also need to add the following dependency:

compileOnly "org.mapstruct:mapstruct-processor:${mapstructVersion}"
If you are using IntelliJ IDEA, you also need to ensure that support for annotation processing is enabled. Open Preferences and navigate to Build, Execute, Deployment | Compiler | Annotations Processors. Verify that the checkbox named Enable annotation processing is selected!

For the product and recommendation microservices, we declare the following dependencies to Spring Data for MongoDB:

implementation 'org.springframework.boot:spring-boot-starter-data-mongodb'
For the review microservice, we declare a dependency to Spring Data for JPA and a JDBC driver for MySQL like this:

implementation 'org.springframework.boot:spring-boot-starter-data-jpa'
implementation 'mysql:mysql-connector-java'
To enable the use of MongoDB and MySQL when running automated integration tests, we will use Testcontainers and its support for JUnit 5, MongoDB, and MySQL. For the product and recommendation microservices, we declare the following test dependencies:

implementation platform('org.testcontainers:testcontainers-bom:1.15.2')
testImplementation 'org.testcontainers:testcontainers'
testImplementation 'org.testcontainers:junit-jupiter'
testImplementation 'org.testcontainers:mongodb'
For the review microservices, we declare the following test dependencies:

implementation platform('org.testcontainers:testcontainers-bom:1.15.2')
testImplementation 'org.testcontainers:testcontainers'
testImplementation 'org.testcontainers:junit-jupiter'
testImplementation 'org.testcontainers:mysql'
For more information on how Testcontainers is used in integration tests, see the Writing automated tests that focus on persistence section later on.

Storing data with entity classes
The entity classes are similar to the corresponding API model classes in terms of what fields they contain; see the Java package se.magnus.api.core in the api project. We will add two fields, id and version, in the entity classes compared to the API model classes.

The id field is used to hold the database identity of each stored entity, corresponding to the primary key when using a relational database. We will delegate the responsibility of generating unique values of the identity field to Spring Data. Depending on the database used, Spring Data can delegate this responsibility to the database engine or handle it on its own. In either case, the application code does not need to consider how a unique database id value is set. The id field is not exposed in the API, as a best practice from a security perspective. The fields in the model classes that identify an entity will be assigned a unique index in the corresponding entity class, to ensure consistency in the database from a business perspective.

The version field is used to implement optimistic locking, allowing Spring Data to verify that updates of an entity in the database do not overwrite a concurrent update. If the value of the version field stored in the database is higher than the value of the version field in an update request, it indicates that the update is performed on stale data—the information to be updated has been updated by someone else since it was read from the database. Attempts to perform updates based on stale data will be prevented by Spring Data. In the section on writing persistence tests, we will see tests verifying that the optimistic locking mechanism in Spring Data prevents updates performed on stale data. Since we only implement APIs for create, read, and delete operations, we will, however, not expose the version field in the API.

The most interesting parts of the product entity class, used for storing entities in MongoDB, look like this:

@Document(collection="products")
public class ProductEntity {

 @Id
 private String id;

 @Version
 private Integer version;

 @Indexed(unique = true)
 private int productId;

 private String name;
 private int weight;
Here are some observations from the preceding code:

The @Document(collection = "products") annotation is used to mark the class as an entity class used for MongoDB, that is, mapped to a collection in MongoDB with the name products.
The @Id and @Version annotations are used to mark the id and version fields to be used by Spring Data, as explained previously.
The @Indexed(unique = true) annotation is used to get a unique index created for the business key, productId.
The most interesting parts of the Recommendation entity class, also used for storing entities in MongoDB, look like this:

@Document(collection="recommendations")
@CompoundIndex(name = "prod-rec-id", unique = true, def = "{'productId': 1, 'recommendationId' : 1}")
public class RecommendationEntity {

    @Id
    private String id;

    @Version
    private Integer version;

    private int productId;
    private int recommendationId;
    private String author;
    private int rating;
    private String content;
Added to the explanations for the preceding product entity, we can see how a unique compound index is created using the @CompoundIndex annotation for the compound business key based on the productId and recommendationId fields.

Finally, the most interesting parts of the Review entity class, used for storing entities in a SQL database like MySQL, look like this:

@Entity
@Table(name = "reviews", indexes = { @Index(name = "reviews_unique_idx", unique = true, columnList = "productId,reviewId") })
public class ReviewEntity {

    @Id @GeneratedValue
    private int id;

    @Version
    private int version;

    private int productId;
    private int reviewId;
    private String author;
    private String subject;
    private String content;
Notes from the preceding code:

The @Entity and @Table annotations are used to mark the class as an entity class used for JPA—mapped to a table in a SQL database with the name reviews.
The @Table annotation is also used to specify that a unique compound index will be created for the compound business key based on the productId and reviewId fields.
The @Id and @Version annotations are used to mark the id and version fields to be used by Spring Data as explained previously. To direct Spring Data for JPA to automatically generate unique id values for the id field, we are using the @GeneratedValue annotation.
For the full source code of the entity classes, see the persistence package in each of the core microservice projects.

Defining repositories in Spring Data
Spring Data comes with a set of base classes for defining repositories. We will use the base classes CrudRepository and PagingAndSortingRepository:

The CrudRepository base class provides standard methods for performing basic create, read, update, and delete operations on the data stored in the databases.
The PagingAndSortingRepository base class adds support for paging and sorting to the CrudRepository base class.
We will use the CrudRepository class as the base class for the Recommendation and Review repositories and the PagingAndSortingRepository class as the base class for the Product repository.

We will also add a few extra query methods to our repositories for looking up entities using the business key, productId.

Spring Data supports defining extra query methods based on naming conventions for the signature of the method. For example, the findByProductId(int productId) method signature can be used to direct Spring Data to automatically create a query that returns entities from the underlying collection or table. In this case, it will return entities that have the productId field set to the value specified in the productId parameter. For more details on how to declare extra queries, see https://docs.spring.io/spring-data/data-commons/docs/current/reference/html/#repositories.query-methods.query-creation.

The Product repository class looks like this:

public interface ProductRepository extends PagingAndSortingRepository <ProductEntity, String> {
    Optional<ProductEntity> findByProductId(int productId);
}
Since the findByProductId method might return zero or one product entity, the return value is marked to be optional by wrapping it in an Optional object.

The Recommendation repository class looks like this:

public interface RecommendationRepository extends CrudRepository <RecommendationEntity, String> {
    List<RecommendationEntity> findByProductId(int productId);
}
In this case, the findByProductId method will return zero to many recommendation entities, so the return value is defined as a list.

Finally, the Review repository class looks like this:

public interface ReviewRepository extends CrudRepository<ReviewEntity, Integer> {
    @Transactional(readOnly = true)
    List<ReviewEntity> findByProductId(int productId);
}
Since SQL databases are transactional, we have to specify the default transaction type—read-only in our case—for the query method, findByProductId().

That's it—this is all it takes to establish a persistence layer for our core microservices.

For the full source code of the repository classes, see the persistence package in each of the core microservice projects.

Let's start using the persistence classes by writing some tests to verify that they work as intended.

Writing automated tests that focus on persistence
When writing persistence tests, we want to start a database when the tests begin and tear it down when the tests complete. However, we don't want the tests to wait for other resources to start up, for example, a web server such as Netty (which is required at runtime).

Spring Boot comes with two class-level annotations tailored for this specific requirement:

@DataMongoTest: This annotation starts up a MongoDB database when the test starts.
@DataJpaTest: This annotation starts up a SQL database when the test starts.
By default, Spring Boot configures the tests to roll back updates to the SQL database to minimize the risk of negative side effects on other tests. In our case, this behavior will cause some of the tests to fail. Therefore, automatic rollback is disabled with the class level annotation @Transactional(propagation = NOT_SUPPORTED).
To handle the startup and tear down of databases during the execution of the integration tests, we will use Testcontainers. Before looking into how to write persistence tests, let's learn about how to use Testcontainers.

Using Testcontainers
Testcontainers (https://www.testcontainers.org) is a library that simplifies running automated integration tests by running resource managers like a database or a message broker as a Docker container. Testcontainers can be configured to automatically start up Docker containers when JUnit tests are started and tear down the containers when the tests are complete.

To enable Testcontainers in an existing test class for a Spring Boot application like the microservices in this book, we can add the @Testcontainers annotation to the test class. Using the @Container annotation, we can for example declare that the Review microservice's integration tests will use a Docker container running MySQL. The code looks like this:

@SpringBootTest
@Testcontainers
class SampleTests {
  @Container
  private static MySQLContainer database = 
    new MySQLContainer("mysql:5.7.32");
The version specified for MySQL, 5.7.32, is copied from Docker Compose files to ensure that the same version is used.

A disadvantage of this approach is that each test class will use its own Docker container. Bringing up MySQL in a Docker container takes a few seconds, typically 10 seconds on my Mac. Running multiple test classes that use the same type of test container will add this latency for each test class. To avoid this extra latency, we can use the Single Container Pattern (see https://www.testcontainers.org/test_framework_integration/manual_lifecycle_control/#singleton-containers). Following this pattern, a base class is used to launch a single Docker container for MySQL. The base class, MySqlTestBase, used in the Review microservice looks like this:

public abstract class MySqlTestBase {

  private static MySQLContainer database =
    new MySQLContainer("mysql:5.7.32");
  
  static {
    database.start();
  }

  @DynamicPropertySource
  static void databaseProperties(DynamicPropertyRegistry registry) {
    registry.add("spring.datasource.url", database::getJdbcUrl);
    registry.add("spring.datasource.username", database::getUsername);
    registry.add("spring.datasource.password", database::getPassword);
  }
}
Explanations for the preceding source code:

The database container is declared in the same way as in the preceding example.
A static block is used to start the database container before any JUnit code is invoked.
The database container will get some properties defined when started up, such as which port to use. To register these dynamically created properties in the application context, a static method databaseProperties() is defined. The method is annotated with @DynamicPropertySource to override the database configuration in the application context, such as the configuration from an application.yml file.
The test classes use the base class as follows:

class PersistenceTests extends MySqlTestBase {
class ReviewServiceApplicationTests extends MySqlTestBase { 
For the product and review microservices, which use MongoDB, a corresponding base class, MongoDbTestBase, has been added.

By default, the log output from Testcontainers is rather extensive. A Logback configuration file can be placed in the src/test/resource folder to limit the amount of log output. Logback is a logging framework (http://logback.qos.ch), and it is included in the microservices by using the spring-boot-starter-webflux dependency. For details, see https://www.testcontainers.org/supported_docker_environment/logging_config/. The configuration file used in this chapter is named src/test/resources/logback-test.xml and looks like this:

<?xml version="1.0" encoding="UTF-8" ?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/defaults.xml"/>
    <include resource="org/springframework/boot/logging/logback/console-appender.xml"/>

    <root level="INFO">
        <appender-ref ref="CONSOLE" />
    </root>
</configuration>
Some notes from the above XML file:

The config file includes two config files provided by Spring Boot to get default values defined, and a log appender is configured that can write log events to the console.
The config file limits log output to the INFO log level, discarding DEBUG and TRACE log records emitted by the Testcontainers library.
For details on Spring Boot support for logging and the use of Logback, see https://docs.spring.io/spring-boot/docs/current/reference/html/howto.html#howto-configure-logback-for-logging.

Finally, when using the @DataMongoTest and @DataJpaTest annotations instead of the @SpringBootTest annotation to only start up the MongoDB and SQL database during the integration test, there is one more thing to consider. The @DataMongoTest and @DataJpaTest annotations are designed to start an embedded database by default. Since we want to use a containerized database, we have to disable this feature. For the @DataJpaTest annotation, this can be done by using a @AutoConfigureTestDatabase annotation like this:

@DataJpaTest
@AutoConfigureTestDatabase(replace = AutoConfigureTestDatabase.Replace.NONE)
class PersistenceTests extends MySqlTestBase {
For the @DataMongoTest annotation, this can be done by using the excludeAutoConfiguration parameter and specifying that the class EmbeddedMongoAutoConfiguration will be excluded. The code looks like this:

@DataMongoTest(
  excludeAutoConfiguration = EmbeddedMongoAutoConfiguration.class)
class PersistenceTests extends MongoDbTestBase {
With Testcontainers introduced, we are ready to see how persistence tests can be written.

Writing persistence tests
The persistence tests for the three core microservices are similar to each other, so we will only go through the persistence tests for the product microservice.

The test class, PersistenceTests, declares a method, setupDb(), annotated with @BeforeEach, which is executed before each test method. The setup method removes any entities from previous tests in the database and inserts an entity that the test methods can use as a base for their tests:

@DataMongoTest
class PersistenceTests {

    @Autowired
    private ProductRepository repository;
    private ProductEntity savedEntity;

    @BeforeEach
    void setupDb() {
        repository.deleteAll();
        ProductEntity entity = new ProductEntity(1, "n", 1);
        savedEntity = repository.save(entity);
        assertEqualsProduct(entity, savedEntity);
    }
Next come the various test methods. First out is a create test:

@Test
void create() {
    ProductEntity newEntity = new ProductEntity(2, "n", 2);
    repository.save(newEntity);

    ProductEntity foundEntity = 
    repository.findById(newEntity.getId()).get();
    assertEqualsProduct(newEntity, foundEntity);

    assertEquals(2, repository.count());
}
This test creates a new entity, verifies that it can be found using the findById() method, and wraps up by asserting that there are two entities stored in the database, the one created by the setup method and the one created by the test itself.

The update test looks like this:

@Test
void update() {
    savedEntity.setName("n2");
    repository.save(savedEntity);

    ProductEntity foundEntity = 
    repository.findById(savedEntity.getId()).get();
    assertEquals(1, (long)foundEntity.getVersion());
    assertEquals("n2", foundEntity.getName());
}
This test updates the entity created by the setup method, reads it again from the database using the standard findById() method, and asserts that it contains expected values for some of its fields. Note that, when an entity is created, its version field is set to 0 by Spring Data, so we expect it to be 1 after the update.

The delete test looks like this:

@Test
void delete() {
    repository.delete(savedEntity);
    assertFalse(repository.existsById(savedEntity.getId()));
}
This test deletes the entity created by the setup method and verifies that it no longer exists in the database.

The read test looks like this:

@Test
void getByProductId() {
    Optional<ProductEntity> entity = 
    repository.findByProductId(savedEntity.getProductId());
    assertTrue(entity.isPresent());
    assertEqualsProduct(savedEntity, entity.get());
}
This test uses the findByProductId() method to get the entity created by the setup method, verifies that it was found, and then uses the local helper method, assertEqualsProduct(), to verify that the entity returned by findByProductId() looks the same as the entity stored by the setup method.

Next are two test methods that verify alternative flows—handling of error conditions. First is a test that verifies that duplicates are handled correctly:

@Test
void duplicateError() {
  assertThrows(DuplicateKeyException.class, () -> {
    ProductEntity entity = new ProductEntity(savedEntity.getProductId(), "n", 1);
    repository.save(entity);
  });
} 
The test tries to store an entity with the same business key as used by the entity created by the setup method. The test will fail if the save operation succeeds, or if the save fails with an exception other than the expected DuplicateKeyException.

The other negative test is, in my opinion, the most interesting test in the test class. It is a test that verifies a correct error handling in the case of updates of stale data—it verifies that the optimistic locking mechanism works. It looks like this:

@Test
void optimisticLockError() {

    // Store the saved entity in two separate entity objects
    ProductEntity entity1 = 
    repository.findById(savedEntity.getId()).get();
    ProductEntity entity2 = 
    repository.findById(savedEntity.getId()).get();

    // Update the entity using the first entity object
    entity1.setName("n1");
    repository.save(entity1);

    //  Update the entity using the second entity object.
    // This should fail since the second entity now holds an old version 
    // number, that is, an Optimistic Lock Error
    assertThrows(OptimisticLockingFailureException.class, () -> {
      entity2.setName("n2");
      repository.save(entity2);
    });

    // Get the updated entity from the database and verify its new state
    ProductEntity updatedEntity = 
    repository.findById(savedEntity.getId()).get();
    assertEquals(1, (int)updatedEntity.getVersion());
    assertEquals("n1", updatedEntity.getName());
}


@Test
void paging() {
    repository.deleteAll();
    List<ProductEntity> newProducts = rangeClosed(1001, 1010)
        .mapToObj(i -> new ProductEntity(i, "name " + i, i))
        .collect(Collectors.toList());
    repository.saveAll(newProducts);

    Pageable nextPage = PageRequest.of(0, 4, ASC, "productId");
    nextPage = testNextPage(nextPage, "[1001, 1002, 1003, 1004]", 
    true);
    nextPage = testNextPage(nextPage, "[1005, 1006, 1007, 1008]", 
    true);
    nextPage = testNextPage(nextPage, "[1009, 1010]", false);
}


private Pageable testNextPage(Pageable nextPage, String expectedProductIds, boolean expectsNextPage) {
    Page<ProductEntity> productPage = repository.findAll(nextPage);
    assertEquals(expectedProductIds, productPage.getContent()
    .stream().map(p -> p.getProductId()).collect(Collectors.
    toList()).toString());
    assertEquals(expectsNextPage, productPage.hasNext());
    return productPage.nextPageable();
}