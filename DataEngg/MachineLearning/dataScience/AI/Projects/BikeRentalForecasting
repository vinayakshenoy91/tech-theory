1) Loading lib and util fn:
----------------------------

import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler //Used for scaling and standardising the column
from sklearn.metrics import mean_squared_error
from sklearn import linear_model
from matplotlib import pyplot as plt
import os

2) Loading data:
------------------
filePath = '/cxldata/datasets/project/bikes.csv'
dfname = pd.read_csv(filePath)

3) Cleaning data:
-----------------
a) Dropping columns:
dfname=dfname.drop(columnsToDrop,axis=1)

4) Dividing data into train and test:
-------------------------------------
//train_test_split() function uses 'Random Sampling', hence resulting train_set 
//and test_set data sets have to be sorted by dayCount
from sklearn.model_selection import train_test_split
bikesData['dayCount'] = pd.Series(range(bikesData.shape[0]))/24

train_set, test_set = train_test_split(bikesData, test_size=0.3, random_state=42)
train_set.sort_values('dayCount', axis= 0, inplace=True)
test_set.sort_values('dayCount', axis= 0, inplace=True)

4) Basic - Cleaning the data - Feature Scaling:
------------------------------------------------
columnsToScale=['temp', 'hum', 'windspeed']

scaler = StandardScaler()
train_set[columnsToScale] = scaler.fit_transform(train_set[columnsToScale])
test_set[columnsToScale] = scaler.transform(test_set[columnsToScale])

5) Basic - Train and Analyze the Models - Preparing to Train the Models:
--------------------------------------------------------------------------
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

trainingLabels=train_set['cnt']
trainingCols=train_set.drop(['cnt'],axis=1)

#Call cross_val_score() function, to perform training and cross validation and to calculate the mean absolute error scores

#Using DecisionTreeRegressor model
dec_reg=DecisionTreeRegressor(random_state = 42)
dt_mse_scores=-cross_val_score(dec_reg, trainingCols, trainingLabels, cv=10, scoring="neg_mean_absolute_error")
display(dt_mse_scores)


#using linear regression model
lin_reg=LinearRegression()
lr_mae_scores = -cross_val_score(lin_reg,trainingCols,trainingLabels,cv=10, scoring="neg_mean_absolute_error")
display_scores(lr_mae_scores)

lr_mse_scores = np.sqrt(-cross_val_score(lin_reg,trainingCols,trainingLabels,cv=10, scoring="neg_mean_squared_error"))
display_scores(lr_mse_scores)


#using Train Random Forest Model
forest_reg=RandomForestRegressor(random_state=42, n_estimators=150)

rf_mae_scores = -cross_val_score(forest_reg,trainingCols,trainingLabels,cv=10, scoring="neg_mean_absolute_error")
display_scores(rf_mae_scores)

rf_mse_scores = np.sqrt(-cross_val_score(forest_reg,trainingCols,trainingLabels,cv=10, scoring="neg_mean_squared_error"))
display_scores(rf_mse_scores)


Choosing the right model:

Once you have calculated Mean Absolute Errors and Mean Squared Errors in each fold of all the models, you would choose the model 
having the minimum errors as well as the models with minimum variance in errors i.e. the model with minimum mean and std of Mean 
Absolute Errors and Mean Squared Errors.

6) Fine tuning models:
----------------------
let us apply Grid Search on this selected model to fine-tune the model (i.e. find the best hyperparameters for this model).

Basic - Fine-Tuning the Selected Model - Choosing set of hyperparameter combinations for Grid Search

from sklearn.model_selection import GridSearchCV
param_grid=[{'n_estimators': [120, 150],'max_features': [10, 12], 'max_depth': [15, 28]}]
grid_search=GridSearchCV(forest_reg,param_grid,cv=5,scoring="neg_mean_squared_error")

grid_search.fit(trainingCols,trainingLabels)
print(grid_search.best_params_)
print(grid_search.best_estimator_)
feature_importances = grid_search.best_estimator_.feature_importances_
print(feature_importances)

7) Evaluate the model on test - Preparing to test the final model on Test dataset:
-------------------------------------------------------------------------------------
final_model=grid_search.best_estimator_
test_set.sort_values('dayCount', axis= 0, inplace=True)
test_x_cols=(test_set.drop(['cnt'],axis=1)).columns.values
X_test = test_set.loc[:,test_x_cols]
test_y_cols='cnt'
y_test=test_set.loc[:,test_y_cols]

test_set.loc[:,'predictedCounts_test'] = final_model.predict(X_test)
final_mse = mean_squared_error(y_test, test_set.loc[:,'predictedCounts_test'])
print(np.sqrt(final_mse))
test_set.describe()

8) Plot the set:
----------------
times = [9,18]
for time in times:
    fig = plt.figure(figsize=(8, 6))
    fig.clf()
    ax = fig.gca()
    test_set_freg_time = test_set[test_set.hr == time]
    test_set_freg_time.plot(kind = 'line', x = 'dayCount', y = 'cnt', ax = ax)
    test_set_freg_time.plot(kind = 'line', x = 'dayCount', y = 'predictedCounts_test', ax =ax)
    plt.show()